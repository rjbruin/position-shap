{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy 01: measuring appearance & bias\n",
    "\n",
    "HOWTO: run the dataset & model cells, then run either of the solution cells following by the analysis cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 training samples\n",
      "216 test samples\n",
      "216 analysis samples\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "\n",
    "# COLORS = [\n",
    "#     ([0.0, 0.0, 0.0], 'black'),\n",
    "#     ([1.0, 0.0, 0.0], 'red'),\n",
    "#     ([0.0, 1.0, 0.0], 'green'),\n",
    "#     ([0.0, 0.0, 1.0], 'blue'),\n",
    "#     ([1.0, 1.0, 0.0], 'yellow'),\n",
    "#     ([1.0, 0.0, 1.0], 'magenta'),\n",
    "#     ([0.0, 1.0, 1.0], 'cyan'),\n",
    "#     ([1.0, 1.0, 1.0], 'white'),\n",
    "# ]\n",
    "\n",
    "# def multicolored_random_positions_toy_dataset(size=[5,5]):\n",
    "#     \"\"\"Create a dataset where each image has one pixel, and the color of the pixel determines the class.\"\"\"\n",
    "#     images = []\n",
    "#     labels = []\n",
    "#     for t, (vals, c) in enumerate(COLORS):\n",
    "#         for y1 in range(size[0]):\n",
    "#             for x1 in range(size[1]):\n",
    "#                 image = torch.rand([3] + size) * 0.0\n",
    "#                 image[0, y1, x1] = vals[0]\n",
    "#                 image[1, y1, x1] = vals[1]\n",
    "#                 image[2, y1, x1] = vals[2]\n",
    "\n",
    "#                 images.append(image)\n",
    "#                 labels.append(t)\n",
    "\n",
    "#     images = torch.stack(images)\n",
    "#     labels = torch.tensor(labels, dtype=torch.long)\n",
    "#     return images, labels\n",
    "\n",
    "# images, labels = multicolored_random_positions_toy_dataset()\n",
    "\n",
    "%autoreload\n",
    "import datasets\n",
    "\n",
    "images, labels, n_classes, sort_groups, = datasets.dataset_mixed_position(0, 6)\n",
    "train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels = datasets.split_dataset(images, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 training samples\n",
      "200 test samples\n",
      "25 analysis samples\n"
     ]
    }
   ],
   "source": [
    "# # DEBUG: test specific sample attribution\n",
    "# analysis_indices = train_labels == 2\n",
    "# analysis_images = train_images[analysis_indices]\n",
    "# analysis_labels = train_labels[analysis_indices]\n",
    "\n",
    "# print(f\"{train_images.shape[0]} training samples\")\n",
    "# print(f\"{test_images.shape[0]} test samples\")\n",
    "# print(f\"{analysis_images.shape[0]} analysis samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2040x240 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# plt.style.use(['seaborn'])\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"sans-serif\",\n",
    "#     \"font.sans-serif\": [\"Palatino\"]})\n",
    "# matplotlib.rcParams['mathtext.fontset'] = 'stix'\n",
    "# matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "\n",
    "# n_colors = n_classes\n",
    "# fig, axs = plt.subplots(1, 2 * n_colors, figsize=(1 + 2 * n_colors, 2), dpi=120)\n",
    "# j = 0\n",
    "# for c in range(n_classes):\n",
    "#     for i in range(2):\n",
    "#         inds = train_labels == c\n",
    "#         axs[j].imshow(train_images[inds][i].permute((1, 2, 0)))\n",
    "#         axs[j].set_title(f\"Class {train_labels[inds][i]}\")\n",
    "#         # inds = analysis_labels == c\n",
    "#         # if len(analysis_images[inds]) > 0:\n",
    "#         #     axs[j].imshow(analysis_images[inds][i].permute((1, 2, 0)))\n",
    "#         #     axs[j].set_title(f\"Class {analysis_labels[inds][i]}\")\n",
    "#         axs[j].axis('off')\n",
    "#         j += 1\n",
    "\n",
    "# plt.tight_layout()\n",
    "# pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert-jan/surfdrive/experiments/vit-position-info/toy-experiments/../models/irpe.py:14: UserWarning: \u001b[91m[WARNING] The module `rpe_ops` is not built. For better training performance, please build `rpe_ops`.\u001b[00m\n",
      "  warnings.warn(RED_STR.format(\"[WARNING] The module `rpe_ops` is not built. \\\n"
     ]
    }
   ],
   "source": [
    "# %autoreload\n",
    "\n",
    "# from torch import nn\n",
    "# import ml_collections\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('..')\n",
    "\n",
    "# from models.vit_modeling import Block\n",
    "\n",
    "# def block_config(d=4, n_heads=1):\n",
    "#     \"\"\"Returns the vit configuration for the cifar dataset\"\"\"\n",
    "#     config = ml_collections.ConfigDict()\n",
    "#     config.patches = ml_collections.ConfigDict({'size': (1, 1)})\n",
    "#     config.hidden_size = d\n",
    "#     config.transformer = ml_collections.ConfigDict()\n",
    "#     config.transformer.mlp_dim = d\n",
    "#     config.transformer.num_heads = n_heads\n",
    "#     # config.transformer.num_heads = 2\n",
    "#     config.transformer.num_layers = 1\n",
    "#     config.transformer.attention_dropout_rate = 0.0\n",
    "#     config.transformer.dropout_rate = 0.0\n",
    "#     return config\n",
    "\n",
    "# class TriViTal(nn.Module):\n",
    "#     def __init__(self, d=4, n_heads=1, handcrafted_weights=None, pool='avg', norm=True, residual=True, pos_emb='absolute', use_rel_pos=True, n_classes=2):\n",
    "#         super().__init__()\n",
    "#         self.patch_layer = nn.Conv2d(3, d, kernel_size=1, padding=0)\n",
    "#         if pos_emb == 'absolute':\n",
    "#             self.pos_embedding = nn.Parameter(torch.randn(1, d, 5, 5) * 0.1)\n",
    "#         elif pos_emb == 'none':\n",
    "#             # self.pos_embedding = torch.zeros(1, d, 5, 5)\n",
    "#             self.pos_embedding = None\n",
    "#         # self.pos_embedding = nn.Parameter(torch.zeros(1, d, 5, 5))\n",
    "#         # self.pos_embedding.requires_grad_(False)\n",
    "#         # self.block1 = Block(block_config(d), True, None, False, False, override_attn_out_dim=2)\n",
    "#         self.block1 = Block(config=block_config(d, n_heads=n_heads), img_size=(5, 5), use_rel_pos=use_rel_pos, compute_gradbased_attr=True, norm=norm, residual=residual)\n",
    "#         if pool == 'avg':\n",
    "#             self.pool = nn.AdaptiveAvgPool1d([1])\n",
    "#         elif pool == 'max':\n",
    "#             self.pool = nn.AdaptiveMaxPool1d([1])\n",
    "#         else:\n",
    "#             raise ValueError(f\"Unknown pooling type: {pool}\")\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.fc = nn.Linear(d, n_classes)\n",
    "\n",
    "#         self.image_in = None\n",
    "#         self.pos_emb_in = None\n",
    "#         self.attn_probs = None\n",
    "#         self.attn_out = None\n",
    "#         self.attn_probs_sliced = []\n",
    "\n",
    "#         if handcrafted_weights is not None:\n",
    "#             params = dict(self.named_parameters())\n",
    "#             training = {n: True for n in params.keys()}\n",
    "#             for name, param in handcrafted_weights.items():\n",
    "#                 params[name].data = handcrafted_weights[name]\n",
    "#                 training[name] = False\n",
    "\n",
    "#             # print('\\nTraining:')\n",
    "#             # for name in training:\n",
    "#             #     print(f\"{name}: {training[name]}\")\n",
    "\n",
    "#             if all([not training[name] for name in params]):\n",
    "#                 print('\\nNo parameters to train!')\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.image_in = x\n",
    "#         self.image_in.requires_grad = True\n",
    "#         self.image_in.retain_grad()\n",
    "\n",
    "#         def to_attn_format(data):\n",
    "#             return data.permute((0, 2, 3, 1)).view(data.shape[0], -1, data.shape[1])\n",
    "#         def from_attn_format(data):\n",
    "#             return data.permute([0, 2, 1])\n",
    "\n",
    "#         out_sem = self.patch_layer(x)\n",
    "#         self.patch_activations = out_sem\n",
    "#         out_pos = self.pos_embedding\n",
    "#         self.pos_emb_in = out_pos\n",
    "#         if self.pos_embedding is not None:\n",
    "#             out = out_sem + out_pos\n",
    "#         else:\n",
    "#             out = out_sem\n",
    "#         self.token_activations = out\n",
    "\n",
    "#         out = to_attn_format(out)\n",
    "#         out, self.attn_probs, self.attn_probs_sliced = self.block1(out)\n",
    "\n",
    "#         # To make feature maps differentiable *per sample* we\n",
    "#         # need to include the slicing per sample in the graph. Otherwise\n",
    "#         # autograd.grad() complains that the sliced tensors are not in the\n",
    "#         # graph when we later compute the image/position attributions wrt\n",
    "#         # the feature map.\n",
    "#         self.attn_out_sliced = []\n",
    "#         # Shape of feature map: [batch_size, tokens, channels]\n",
    "#         B, D, C = out.shape\n",
    "#         for i in range(B):\n",
    "#             # What we want: first dimension should be [num_heads], so\n",
    "#             # autograd.grad() can compute the \"batched\" vector-Jacobian for each\n",
    "#             # head, as we will use the \"batched\" mode of grad() where the first\n",
    "#             # dimension needs to contain the \"batches\".\n",
    "#             num_heads = self.block1.attn.num_attention_heads\n",
    "#             grad_compat_slice = out[i].permute((1, 0)).reshape(num_heads, -1, D)\n",
    "#             grad_compat_slice.retain_grad()\n",
    "#             self.attn_out_sliced.append(grad_compat_slice)\n",
    "#         out = torch.stack(self.attn_out_sliced).reshape(B, C, D).permute((0, 2, 1))\n",
    "\n",
    "#         out = from_attn_format(out)\n",
    "\n",
    "#         out = self.pool(out)\n",
    "#         out = self.flatten(out)\n",
    "#         out = self.fc(out)\n",
    "\n",
    "#         # DEBUG\n",
    "#         self.logits = out\n",
    "\n",
    "#         return out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert-jan/surfdrive/experiments/vit-position-info/models/irpe.py:14: UserWarning: \u001b[91m[WARNING] The module `rpe_ops` is not built. For better training performance, please build `rpe_ops`.\u001b[00m\n",
      "  warnings.warn(RED_STR.format(\"[WARNING] The module `rpe_ops` is not built. \\\n"
     ]
    }
   ],
   "source": [
    "# %autoreload\n",
    "# import numpy as np\n",
    "\n",
    "# import training\n",
    "# import utils\n",
    "\n",
    "\n",
    "# def run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=250, train=True, model=None, attribution_method='input_gradient'):\n",
    "#     biases = {'bias': [], 'appearance': [], 'position': [], 'relative_position': []}\n",
    "#     biases_withbias = {'bias': [], 'appearance': [], 'position': [], 'relative_position': []}\n",
    "#     cls_biases = {c: {'bias': [], 'appearance': [], 'position': [], 'relative_position': []} for c in range(n_classes)}\n",
    "#     cls_biases_withbias = {c: {'bias': [], 'appearance': [], 'position': [], 'relative_position': []} for c in range(n_classes)}\n",
    "#     for seed in seeds:\n",
    "#         torch.manual_seed(seed)\n",
    "#         # DEBUG\n",
    "#         if train:\n",
    "#             model = TriViTal(d=d, n_heads=n_heads, use_rel_pos=use_rel_pos, pos_emb=pos_emb, n_classes=n_classes)\n",
    "#             training.train_toy(model, train_images, train_labels, test_images, test_labels, epochs=n_epochs, report_every_n=report_every_n, batch_size=128, lr=lr)\n",
    "\n",
    "#         sources_available = ['image', 'bias']\n",
    "#         if pos_emb != 'none':\n",
    "#             sources_available.append('pos_emb')\n",
    "#         if use_rel_pos:\n",
    "#             sources_available.append('relpos')\n",
    "#         seed_biases, seed_biases_withbias, seed_cls_biases, seed_cls_biases_withbias = \\\n",
    "#             utils.toy_all_analyses(model, analysis_images, analysis_labels, n_classes, seed, sources_available=sources_available, attribution_method=attribution_method)\n",
    "\n",
    "#         for key in seed_biases:\n",
    "#             biases[key].append(seed_biases[key])\n",
    "#             for c in range(n_classes):\n",
    "#                 if c in seed_cls_biases:\n",
    "#                     cls_biases[c][key].append(seed_cls_biases[c][key])\n",
    "\n",
    "#         for key in seed_biases_withbias:\n",
    "#             biases_withbias[key].append(seed_biases_withbias[key])\n",
    "#             for c in range(n_classes):\n",
    "#                 if c in seed_cls_biases_withbias:\n",
    "#                     cls_biases_withbias[c][key].append(seed_cls_biases_withbias[c][key])\n",
    "\n",
    "#     # utils.toy_postprocess_analysis(biases, biases_withbias, cls_biases, cls_biases_withbias, seeds, n_classes, sort_by_appearance=True)\n",
    "#     utils.toy_postprocess_analysis(biases, biases_withbias, cls_biases, cls_biases_withbias, seeds, n_classes, sort_by_appearance=False)\n",
    "\n",
    "#     return model\n",
    "\n",
    "%autoreload\n",
    "from experiments import run as run_appearance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No PE - including black - fix zero grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.805870771408081\n",
      "Epoch 999: 1.8431199350743555e-05\n",
      "Accuracy: 1.0\n",
      "Sorting by appearance for these groups of classes:  [range(0, 6)]\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 100.00 +- 0.00 (100.00)\n",
      "appearance (c0) : 100.00 +- 0.00 (100.00)\n",
      "appearance (c1) : 100.00 +- 0.00 (100.00)\n",
      "appearance (c2) : 100.00 +- 0.00 (100.00)\n",
      "appearance (c3) : 100.00 +- 0.00 (100.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00)\n",
      "appearance (c5) : 100.00 +- 0.00 (100.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 96.25 +- 0.00 (96.25)\n",
      "bias (c0) : 98.91 +- 0.00 (98.91)\n",
      "bias (c1) : 98.01 +- 0.00 (98.01)\n",
      "bias (c2) : 96.57 +- 0.00 (96.57)\n",
      "bias (c3) : 95.77 +- 0.00 (95.77)\n",
      "bias (c4) : 95.02 +- 0.00 (95.02)\n",
      "bias (c5) : 93.19 +- 0.00 (93.19)\n",
      "appearance (all): 3.75 +- 0.00 (3.75)\n",
      "appearance (c0) : 1.09 +- 0.00 (1.09)\n",
      "appearance (c1) : 1.99 +- 0.00 (1.99)\n",
      "appearance (c2) : 3.43 +- 0.00 (3.43)\n",
      "appearance (c3) : 4.23 +- 0.00 (4.23)\n",
      "appearance (c4) : 4.98 +- 0.00 (4.98)\n",
      "appearance (c5) : 6.81 +- 0.00 (6.81)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "# n_epochs = 4000\n",
    "# report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = False\n",
    "attribution_method = 'input_gradient_withnegative'\n",
    "target = 'pred_class'\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "report_every_n = 1000\n",
    "seeds = [1]\n",
    "d = 8\n",
    "n_heads = 4\n",
    "\n",
    "model = run_appearance('appearance', seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n, attribution_method=attribution_method, target=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No PE - including black - attribution with negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.0763132572174072\n",
      "Epoch 999: 0.0001665823074290529\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2209866046905518\n",
      "Epoch 999: 6.217013287823647e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.195828914642334\n",
      "Epoch 999: 6.830450729466975e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.206942319869995\n",
      "Epoch 999: 0.00019098276970908046\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1864943504333496\n",
      "Epoch 999: 0.00016232547932304442\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.113070011138916\n",
      "Epoch 999: 0.00011727566015906632\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.132812023162842\n",
      "Epoch 999: 5.211434108787216e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.088918924331665\n",
      "Epoch 999: 8.187897765310481e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2273597717285156\n",
      "Epoch 999: 0.00020894600311294198\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1424968242645264\n",
      "Epoch 999: 0.00022861125762574375\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 87.50 +- 0.00 (87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c2) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c3) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c5) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c6) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c7) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 78.28 +- 6.88 (79.93, 79.84, 90.88, 74.57, 77.49, 67.02, 71.72, 77.31, 75.08, 88.95)\n",
      "bias (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "bias (c1) : 84.79 +- 9.69 (81.14, 92.01, 99.11, 73.93, 98.41, 81.86, 73.93, 91.40, 70.84, 85.26)\n",
      "bias (c2) : 86.53 +- 12.67 (99.53, 98.21, 99.46, 90.95, 72.04, 67.70, 69.95, 90.90, 77.13, 99.45)\n",
      "bias (c3) : 82.62 +- 10.61 (77.43, 99.89, 98.06, 79.49, 76.19, 73.94, 89.86, 86.27, 63.43, 81.60)\n",
      "bias (c4) : 61.85 +- 18.81 (86.64, 72.03, 76.06, 51.64, 51.14, 36.65, 32.16, 63.04, 57.87, 91.26)\n",
      "bias (c5) : 78.89 +- 10.15 (71.02, 71.32, 88.15, 61.79, 80.51, 74.78, 89.81, 69.25, 90.92, 91.39)\n",
      "bias (c6) : 73.55 +- 17.02 (62.20, 62.30, 89.67, 91.38, 87.53, 63.41, 65.75, 39.04, 78.49, 95.67)\n",
      "bias (c7) : 58.00 +- 12.91 (61.50, 42.95, 76.52, 47.35, 54.13, 37.84, 52.26, 78.55, 61.96, 66.97)\n",
      "appearance (all): 21.72 +- 6.88 (20.07, 20.16, 9.12, 25.43, 22.51, 32.98, 28.28, 22.69, 24.92, 11.05)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 15.21 +- 9.69 (18.86, 7.99, 0.89, 26.07, 1.59, 18.14, 26.07, 8.60, 29.16, 14.74)\n",
      "appearance (c2) : 13.47 +- 12.67 (0.47, 1.79, 0.54, 9.05, 27.96, 32.30, 30.05, 9.10, 22.87, 0.55)\n",
      "appearance (c3) : 17.38 +- 10.61 (22.57, 0.11, 1.94, 20.51, 23.81, 26.06, 10.14, 13.73, 36.57, 18.40)\n",
      "appearance (c4) : 38.15 +- 18.81 (13.36, 27.97, 23.94, 48.36, 48.86, 63.35, 67.84, 36.96, 42.13, 8.74)\n",
      "appearance (c5) : 21.11 +- 10.15 (28.98, 28.68, 11.85, 38.21, 19.49, 25.22, 10.19, 30.75, 9.08, 8.61)\n",
      "appearance (c6) : 26.45 +- 17.02 (37.80, 37.70, 10.33, 8.62, 12.47, 36.59, 34.25, 60.96, 21.51, 4.33)\n",
      "appearance (c7) : 42.00 +- 12.91 (38.50, 57.05, 23.48, 52.65, 45.87, 62.16, 47.74, 21.45, 38.04, 33.03)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = False\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "# seeds = [1]\n",
    "d = 4\n",
    "n_heads = 2\n",
    "\n",
    "\n",
    "model = run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No PE - including black - attribution without negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.0763132572174072\n",
      "Epoch 999: 0.0001665823074290529\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2209866046905518\n",
      "Epoch 999: 6.217013287823647e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.195828914642334\n",
      "Epoch 999: 6.830450729466975e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.206942319869995\n",
      "Epoch 999: 0.00019098276970908046\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1864943504333496\n",
      "Epoch 999: 0.00016232547932304442\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.113070011138916\n",
      "Epoch 999: 0.00011727566015906632\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.132812023162842\n",
      "Epoch 999: 5.211434108787216e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.088918924331665\n",
      "Epoch 999: 8.187897765310481e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2273597717285156\n",
      "Epoch 999: 0.00020894600311294198\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1424968242645264\n",
      "Epoch 999: 0.00022861125762574375\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 68.75 +- 11.52 (75.00, 75.00, 87.50, 50.00, 62.50, 87.50, 62.50, 62.50, 62.50, 62.50)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 40.00 +- 48.99 (0.00, 0.00, 100.00, 0.00, 0.00, 100.00, 0.00, 0.00, 100.00, 100.00)\n",
      "appearance (c2) : 60.00 +- 48.99 (100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 0.00, 0.00, 100.00, 0.00)\n",
      "appearance (c3) : 70.00 +- 45.83 (100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 0.00, 0.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c5) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c6) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 0.00, 100.00)\n",
      "appearance (c7) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 79.06 +- 8.57 (74.75, 81.88, 91.37, 82.65, 77.83, 59.97, 72.66, 79.08, 79.67, 90.76)\n",
      "bias (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "bias (c1) : 88.20 +- 18.99 (100.00, 100.00, 98.49, 100.00, 100.00, 64.71, 100.00, 100.00, 44.37, 74.40)\n",
      "bias (c2) : 85.09 +- 22.29 (99.05, 97.83, 99.29, 100.00, 56.72, 41.48, 100.00, 100.00, 56.58, 100.00)\n",
      "bias (c3) : 79.94 +- 22.28 (56.87, 99.78, 96.62, 100.00, 58.41, 35.55, 82.64, 69.47, 100.00, 100.00)\n",
      "bias (c4) : 63.15 +- 17.55 (75.24, 74.80, 76.15, 62.89, 49.86, 36.76, 34.25, 54.48, 80.84, 86.19)\n",
      "bias (c5) : 80.80 +- 17.52 (35.69, 74.59, 90.52, 85.57, 100.00, 71.73, 79.77, 79.48, 96.35, 94.33)\n",
      "bias (c6) : 77.61 +- 22.16 (78.18, 61.81, 92.25, 76.05, 99.20, 92.60, 40.39, 38.87, 100.00, 96.79)\n",
      "bias (c7) : 57.71 +- 17.19 (52.94, 46.24, 77.68, 36.69, 58.46, 36.94, 44.19, 90.37, 59.23, 74.36)\n",
      "appearance (all): 20.94 +- 8.57 (25.25, 18.12, 8.63, 17.35, 22.17, 40.03, 27.34, 20.92, 20.33, 9.24)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 11.80 +- 18.99 (0.00, 0.00, 1.51, 0.00, 0.00, 35.29, 0.00, 0.00, 55.63, 25.60)\n",
      "appearance (c2) : 14.91 +- 22.29 (0.95, 2.17, 0.71, 0.00, 43.28, 58.52, 0.00, 0.00, 43.42, 0.00)\n",
      "appearance (c3) : 20.06 +- 22.28 (43.13, 0.22, 3.38, 0.00, 41.59, 64.45, 17.36, 30.53, 0.00, 0.00)\n",
      "appearance (c4) : 36.85 +- 17.55 (24.76, 25.20, 23.85, 37.11, 50.14, 63.24, 65.75, 45.52, 19.16, 13.81)\n",
      "appearance (c5) : 19.20 +- 17.52 (64.31, 25.41, 9.48, 14.43, 0.00, 28.27, 20.23, 20.52, 3.65, 5.67)\n",
      "appearance (c6) : 22.39 +- 22.16 (21.82, 38.19, 7.75, 23.95, 0.80, 7.40, 59.61, 61.13, 0.00, 3.21)\n",
      "appearance (c7) : 42.29 +- 17.19 (47.06, 53.76, 22.32, 63.31, 41.54, 63.06, 55.81, 9.63, 40.77, 25.64)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3432: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:265: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:223: RuntimeWarning: invalid value encountered in divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "/home/robert-jan/miniconda3/envs/vit/lib/python3.10/site-packages/numpy/core/_methods.py:257: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = False\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "# seeds = [1]\n",
    "d = 4\n",
    "n_heads = 2\n",
    "\n",
    "\n",
    "model = run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No PE - attribution including negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.0228934288024902\n",
      "Epoch 999: 7.121571979951113e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.9618781805038452\n",
      "Epoch 999: 0.00011866469139931723\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.111234188079834\n",
      "Epoch 999: 9.630551357986405e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.0839877128601074\n",
      "Epoch 999: 0.00012371964112389833\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.989585041999817\n",
      "Epoch 999: 0.00011145565076731145\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.92488694190979\n",
      "Epoch 999: 6.568679236806929e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.954270839691162\n",
      "Epoch 999: 6.034078978700563e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.009945869445801\n",
      "Epoch 999: 8.972497744252905e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.046945810317993\n",
      "Epoch 999: 0.00018516351701691747\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1842968463897705\n",
      "Epoch 999: 0.0003004201571457088\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c1) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c2) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c3) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c5) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c6) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 73.84 +- 6.23 (71.77, 79.40, 76.59, 69.79, 61.70, 67.81, 73.02, 74.26, 79.91, 84.18)\n",
      "bias (c0) : 84.49 +- 11.81 (82.96, 99.83, 87.58, 81.27, 87.04, 63.50, 66.91, 99.89, 80.24, 95.68)\n",
      "bias (c1) : 86.02 +- 15.87 (98.36, 97.86, 90.48, 69.64, 64.80, 99.86, 55.09, 86.00, 99.56, 98.57)\n",
      "bias (c2) : 78.83 +- 9.50 (95.81, 83.76, 81.95, 70.58, 61.02, 73.70, 82.39, 84.62, 84.92, 69.54)\n",
      "bias (c3) : 71.48 +- 9.59 (59.90, 62.89, 78.26, 55.67, 71.47, 80.91, 87.01, 73.90, 78.65, 66.16)\n",
      "bias (c4) : 73.41 +- 15.69 (73.96, 92.78, 87.33, 73.77, 36.63, 86.05, 58.12, 66.01, 82.12, 77.29)\n",
      "bias (c5) : 66.17 +- 16.55 (46.14, 64.52, 79.28, 78.17, 67.64, 28.57, 69.37, 67.05, 70.93, 90.05)\n",
      "bias (c6) : 56.48 +- 19.84 (45.22, 54.15, 31.27, 59.40, 43.30, 42.05, 92.22, 42.31, 62.96, 91.95)\n",
      "appearance (all): 26.16 +- 6.23 (28.23, 20.60, 23.41, 30.21, 38.30, 32.19, 26.98, 25.74, 20.09, 15.82)\n",
      "appearance (c0) : 15.51 +- 11.81 (17.04, 0.17, 12.42, 18.73, 12.96, 36.50, 33.09, 0.11, 19.76, 4.32)\n",
      "appearance (c1) : 13.98 +- 15.87 (1.64, 2.14, 9.52, 30.36, 35.20, 0.14, 44.91, 14.00, 0.44, 1.43)\n",
      "appearance (c2) : 21.17 +- 9.50 (4.19, 16.24, 18.05, 29.42, 38.98, 26.30, 17.61, 15.38, 15.08, 30.46)\n",
      "appearance (c3) : 28.52 +- 9.59 (40.10, 37.11, 21.74, 44.33, 28.53, 19.09, 12.99, 26.10, 21.35, 33.84)\n",
      "appearance (c4) : 26.59 +- 15.69 (26.04, 7.22, 12.67, 26.23, 63.37, 13.95, 41.88, 33.99, 17.88, 22.71)\n",
      "appearance (c5) : 33.83 +- 16.55 (53.86, 35.48, 20.72, 21.83, 32.36, 71.43, 30.63, 32.95, 29.07, 9.95)\n",
      "appearance (c6) : 43.52 +- 19.84 (54.78, 45.85, 68.73, 40.60, 56.70, 57.95, 7.78, 57.69, 37.04, 8.05)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = False\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "# seeds = [1]\n",
    "d = 4\n",
    "n_heads = 2\n",
    "\n",
    "\n",
    "model = run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bias': tensor(0.6871), 'appearance': tensor(0.3129)}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "from utils import toy_analysis\n",
    "\n",
    "\n",
    "debug_images = analysis_images[-2:-1]\n",
    "debug_labels = analysis_labels[-2:-1]\n",
    "\n",
    "# DEBUG\n",
    "# debug_images[0, 2, 4, 1] = 0.5\n",
    "debug_images[0, 2, 4, 1] = 1.0\n",
    "print(debug_images[0])\n",
    "\n",
    "seed = 1\n",
    "biases = toy_analysis(['pred_class'], 'scalar', 1, seed, None, model, debug_images, debug_labels, sources_available=['image', 'bias'], exclude_bias=False, attribution_method='input_gradient_withnegative')\n",
    "\n",
    "biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image saliency sum -17.901798\n",
      "model.image_in[0] tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]], grad_fn=<SelectBackward0>)\n",
      "model.image_in.grad[0] tensor([[[  0.1354,   0.1354,   0.1354,   0.1354,   0.1354],\n",
      "         [  0.1354,   0.1354,   0.1354,   0.1354,   0.1354],\n",
      "         [  0.1354,   0.1354,   0.1354,   0.1354,   0.1354],\n",
      "         [  0.1354,   0.1354,   0.1354,   0.1354,   0.1354],\n",
      "         [  0.1354,  51.2197,   0.1354,   0.1354,   0.1354]],\n",
      "\n",
      "        [[ -0.2684,  -0.2684,  -0.2684,  -0.2684,  -0.2684],\n",
      "         [ -0.2684,  -0.2684,  -0.2684,  -0.2684,  -0.2684],\n",
      "         [ -0.2684,  -0.2684,  -0.2684,  -0.2684,  -0.2684],\n",
      "         [ -0.2684,  -0.2684,  -0.2684,  -0.2684,  -0.2684],\n",
      "         [ -0.2684, -53.0297,  -0.2684,  -0.2684,  -0.2684]],\n",
      "\n",
      "        [[  0.0768,   0.0768,   0.0768,   0.0768,   0.0768],\n",
      "         [  0.0768,   0.0768,   0.0768,   0.0768,   0.0768],\n",
      "         [  0.0768,   0.0768,   0.0768,   0.0768,   0.0768],\n",
      "         [  0.0768,   0.0768,   0.0768,   0.0768,   0.0768],\n",
      "         [  0.0768, -17.9018,   0.0768,   0.0768,   0.0768]]])\n",
      "image saliency tensor([[[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
      "\n",
      "        [[ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
      "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
      "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
      "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
      "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000]],\n",
      "\n",
      "        [[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "         [  0.0000, -17.9018,   0.0000,   0.0000,   0.0000]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "model.patch_layer.weight) Parameter containing:\n",
      "tensor([[[[-0.0568]],\n",
      "\n",
      "         [[-0.1139]],\n",
      "\n",
      "         [[ 0.3258]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6041]],\n",
      "\n",
      "         [[-0.7129]],\n",
      "\n",
      "         [[ 0.0190]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1615]],\n",
      "\n",
      "         [[ 0.1736]],\n",
      "\n",
      "         [[-0.2060]]]], requires_grad=True)\n",
      "model.patch_layer.weight.grad tensor([[[[  0.0000]],\n",
      "\n",
      "         [[  0.0000]],\n",
      "\n",
      "         [[-68.3291]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000]],\n",
      "\n",
      "         [[  0.0000]],\n",
      "\n",
      "         [[ 81.9947]]],\n",
      "\n",
      "\n",
      "        [[[  0.0000]],\n",
      "\n",
      "         [[  0.0000]],\n",
      "\n",
      "         [[-13.5970]]]])\n",
      "model.patch_layer saliency tensor([ 6.7326, 12.2840, -0.4155], grad_fn=<MulBackward0>)\n",
      "logits tensor([[ 5.3480, -7.8565, 21.9409, -6.5923, 13.0467, -2.4021, -7.7761]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "biases named saliency [('patch_layer.bias', tensor([ 6.7326, 12.2840, -0.4155], grad_fn=<MulBackward0>)), ('block1.attention_norm.bias', tensor([ 0.3251,  3.9426, -1.1184], grad_fn=<MulBackward0>)), ('block1.ffn_norm.bias', tensor([-1.4156,  4.3810,  0.7223], grad_fn=<MulBackward0>)), ('block1.ffn.fc1.bias', tensor([-1.9085e-01,  1.5439e+00, -1.1968e-03], grad_fn=<MulBackward0>)), ('block1.ffn.fc2.bias', tensor([ 0.1746,  0.0686, -0.2694], grad_fn=<MulBackward0>)), ('block1.attn.query.bias', tensor([-1.2460e-06, -3.4220e-07, -6.3310e-06], grad_fn=<MulBackward0>)), ('block1.attn.key.bias', tensor([ 6.9055e-07,  1.1421e-07, -6.6014e-07], grad_fn=<MulBackward0>)), ('block1.attn.value.bias', tensor([ 0.9735, -0.5975,  0.4533], grad_fn=<MulBackward0>)), ('block1.attn.out.bias', tensor([-1.3032,  1.6176, -0.7013], grad_fn=<MulBackward0>)), ('fc.bias', tensor([ 0.0000, -0.0000, -0.0741,  0.0000, -0.0000, -0.0000,  0.0000],\n",
      "       grad_fn=<MulBackward0>))]\n",
      "bias saliency sum 27.132050607426493\n"
     ]
    }
   ],
   "source": [
    "assert np.allclose(debug_images, model.image_in.detach().numpy())\n",
    "\n",
    "print('image saliency sum', torch.sum(model.image_in[0] * model.image_in.grad[0]).detach().numpy())\n",
    "print('model.image_in[0]', model.image_in[0])\n",
    "print('model.image_in.grad[0]', model.image_in.grad[0])\n",
    "print('image saliency', model.image_in[0] * model.image_in.grad[0])\n",
    "print('model.patch_layer.weight)', model.patch_layer.weight)\n",
    "print('model.patch_layer.weight.grad', model.patch_layer.weight.grad)\n",
    "print('model.patch_layer saliency', model.patch_layer.bias * model.patch_layer.bias.grad)\n",
    "# print(model.fc.weight)\n",
    "# print(model.fc.weight.grad)\n",
    "# print(model.fc.bias)\n",
    "# print(model.fc.bias.grad)\n",
    "print('logits', model.logits)\n",
    "\n",
    "# Collect all bias parameters from the model's layers\n",
    "biases = []\n",
    "bias_sum = 0.\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name:\n",
    "        biases.append((name, param * param.grad))\n",
    "        bias_sum += torch.sum(param * param.grad).detach().numpy()\n",
    "        # print('bias', name)\n",
    "print('biases named saliency', biases)\n",
    "print('bias saliency sum', bias_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-62.464233\n",
    "tensor([[[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]],\n",
    "\n",
    "        [[0., 1., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.],\n",
    "         [0., 0., 0., 0., 0.]]], grad_fn=<SelectBackward0>)\n",
    "tensor([[[ 1.7915e-01,  1.3286e+02,  1.7915e-01,  1.7915e-01,  1.7915e-01],\n",
    "         [ 1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01],\n",
    "         [ 1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01],\n",
    "         [ 1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01],\n",
    "         [ 1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01,  1.7915e-01]],\n",
    "\n",
    "        [[-3.1252e-01, -1.3650e+02, -3.1252e-01, -3.1252e-01, -3.1252e-01],\n",
    "         [-3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01],\n",
    "         [-3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01],\n",
    "         [-3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01],\n",
    "         [-3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01, -3.1252e-01]],\n",
    "\n",
    "        [[ 4.1984e-02, -6.2464e+01,  4.1984e-02,  4.1984e-02,  4.1984e-02],\n",
    "         [ 4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02],\n",
    "         [ 4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02],\n",
    "         [ 4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02],\n",
    "         [ 4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02,  4.1984e-02]]])\n",
    "tensor([[[  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]],\n",
    "\n",
    "        [[ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
    "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
    "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
    "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000],\n",
    "         [ -0.0000,  -0.0000,  -0.0000,  -0.0000,  -0.0000]],\n",
    "\n",
    "        [[  0.0000, -62.4642,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
    "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000]]],\n",
    "       grad_fn=<MulBackward0>)\n",
    "Parameter containing:\n",
    "tensor([[[[-0.0580]],\n",
    "\n",
    "         [[-0.1073]],\n",
    "\n",
    "         [[ 0.3199]]],\n",
    "\n",
    "\n",
    "        [[[ 0.5651]],\n",
    "\n",
    "         [[-0.7452]],\n",
    "\n",
    "         [[ 0.0248]]],\n",
    "\n",
    "\n",
    "        [[[ 0.0434]],\n",
    "\n",
    "         [[ 0.1889]],\n",
    "\n",
    "         [[-0.0945]]]], requires_grad=True)\n",
    "tensor([[[[   0.0000]],\n",
    "\n",
    "         [[   0.0000]],\n",
    "\n",
    "         [[-212.1889]]],\n",
    "\n",
    "\n",
    "        [[[   0.0000]],\n",
    "\n",
    "         [[   0.0000]],\n",
    "\n",
    "         [[ 213.4323]]],\n",
    "\n",
    "\n",
    "        [[[   0.0000]],\n",
    "\n",
    "         [[   0.0000]],\n",
    "\n",
    "         [[  -1.1822]]]])\n",
    "tensor([31.7317, 31.8204, -0.0915], grad_fn=<MulBackward0>)\n",
    "tensor([[ 18.9094, -27.0926,  45.8632,  -4.5033,  37.8864, -23.4203, -19.5874]],\n",
    "       grad_fn=<AddmmBackward0>)\n",
    "[('patch_layer.bias', tensor([31.7317, 31.8204, -0.0915], grad_fn=<MulBackward0>)), ('block1.attention_norm.bias', tensor([3.4664, 6.3488, 0.0365], grad_fn=<MulBackward0>)), ('block1.ffn_norm.bias', tensor([-1.5448,  5.4032,  0.5647], grad_fn=<MulBackward0>)), ('block1.ffn.fc1.bias', tensor([-0.2279,  1.6848, -0.0000], grad_fn=<MulBackward0>)), ('block1.ffn.fc2.bias', tensor([ 0.1930,  0.0141, -0.3650], grad_fn=<MulBackward0>)), ('block1.attn.query.bias', tensor([1.3497e-04, 4.4520e-05, 5.1337e-04], grad_fn=<MulBackward0>)), ('block1.attn.key.bias', tensor([-2.5780e-08, -2.6975e-09,  2.3927e-08], grad_fn=<MulBackward0>)), ('block1.attn.value.bias', tensor([ 2.5504, -2.1057,  0.0168], grad_fn=<MulBackward0>)), ('block1.attn.out.bias', tensor([-0.9058,  3.0259,  0.1915], grad_fn=<MulBackward0>)), ('fc.bias', tensor([ 0.0000, -0.0000, -0.0790,  0.0000, -0.0000, -0.0000,  0.0000],\n",
    "       grad_fn=<MulBackward0>))]\n",
    "81.7292785820212"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 0: 1.9791420698165894\n",
    "Epoch 2000: 1.4175637261359952e-05\n",
    "Epoch 3999: 9.841101018537302e-07\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 1.9975926876068115\n",
    "Epoch 2000: 2.406725616310723e-05\n",
    "Epoch 3999: 1.750091996655101e-06\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.1528146266937256\n",
    "Epoch 2000: 1.8791724869515747e-05\n",
    "Epoch 3999: 9.1548114141915e-05\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.0394437313079834\n",
    "Epoch 2000: 1.7952226698980667e-05\n",
    "Epoch 3999: 9.797645907383412e-05\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.0805203914642334\n",
    "Epoch 2000: 3.612220461945981e-05\n",
    "Epoch 3999: 2.949790768980165e-06\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.042532205581665\n",
    "Epoch 2000: 2.32987804338336e-05\n",
    "Epoch 3999: 1.7221922234966769e-06\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 1.9474270343780518\n",
    "Epoch 2000: 1.1545474990271032e-05\n",
    "Epoch 3999: 9.232375077772303e-07\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.0185437202453613\n",
    "Epoch 2000: 1.4480017853202298e-05\n",
    "Epoch 3999: 1.1337557452861802e-06\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.167551040649414\n",
    "Epoch 2000: 2.691295594559051e-05\n",
    "Epoch 3999: 2.2041008378437255e-06\n",
    "Accuracy: 1.0\n",
    "Epoch 0: 2.0534093379974365\n",
    "Epoch 2000: 1.3876356206310447e-05\n",
    "Epoch 3999: 9.841102155405679e-07\n",
    "Accuracy: 1.0\n",
    "\n",
    "Without bias:\n",
    "appearance (all): 72.86 +- 14.91 (85.71, 57.14, 85.71, 57.14, 57.14, 71.43, 57.14, 71.43, 85.71, 100.00)\n",
    "appearance (c0) : 50.00 +- 50.00 (100.00, 0.00, 100.00, 0.00, 0.00, 0.00, 100.00, 100.00, 0.00, 100.00)\n",
    "appearance (c1) : 60.00 +- 48.99 (100.00, 0.00, 100.00, 0.00, 100.00, 100.00, 0.00, 0.00, 100.00, 100.00)\n",
    "appearance (c2) : 20.00 +- 40.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 100.00, 100.00)\n",
    "appearance (c3) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
    "appearance (c4) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00)\n",
    "appearance (c5) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
    "appearance (c6) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
    "\n",
    "With bias:\n",
    "bias (all): 73.25 +- 6.34 (71.21, 81.69, 60.37, 68.64, 67.46, 81.10, 73.52, 79.41, 73.46, 75.61)\n",
    "bias (c0) : 77.44 +- 30.91 (66.37, 100.00, 9.27, 100.00, 100.00, 100.00, 37.29, 98.17, 100.00, 63.34)\n",
    "bias (c1) : 85.69 +- 26.66 (95.35, 100.00, 84.24, 100.00, 11.91, 66.47, 100.00, 100.00, 99.10, 99.84)\n",
    "bias (c2) : 92.56 +- 15.52 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 52.91, 72.69)\n",
    "bias (c3) : 62.21 +- 21.13 (51.52, 40.38, 33.68, 38.07, 71.89, 97.99, 94.82, 63.60, 68.22, 61.95)\n",
    "bias (c4) : 69.55 +- 24.96 (83.08, 86.77, 91.30, 35.83, 17.17, 76.96, 100.00, 53.59, 82.10, 68.73)\n",
    "bias (c5) : 76.37 +- 17.46 (59.55, 70.55, 95.49, 64.20, 100.00, 93.74, 48.34, 95.05, 61.42, 75.36)\n",
    "bias (c6) : 48.90 +- 21.95 (42.57, 74.14, 8.62, 42.40, 71.22, 32.57, 34.20, 45.44, 50.47, 87.38)\n",
    "appearance (all): 26.75 +- 6.34 (28.79, 18.31, 39.63, 31.36, 32.54, 18.90, 26.48, 20.59, 26.54, 24.39)\n",
    "appearance (c0) : 22.56 +- 30.91 (33.63, 0.00, 90.73, 0.00, 0.00, 0.00, 62.71, 1.83, 0.00, 36.66)\n",
    "appearance (c1) : 14.31 +- 26.66 (4.65, 0.00, 15.76, 0.00, 88.09, 33.53, 0.00, 0.00, 0.90, 0.16)\n",
    "appearance (c2) : 7.44 +- 15.52 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 47.09, 27.31)\n",
    "appearance (c3) : 37.79 +- 21.13 (48.48, 59.62, 66.32, 61.93, 28.11, 2.01, 5.18, 36.40, 31.78, 38.05)\n",
    "appearance (c4) : 30.45 +- 24.96 (16.92, 13.23, 8.70, 64.17, 82.83, 23.04, 0.00, 46.41, 17.90, 31.27)\n",
    "appearance (c5) : 23.63 +- 17.46 (40.45, 29.45, 4.51, 35.80, 0.00, 6.26, 51.66, 4.95, 38.58, 24.64)\n",
    "appearance (c6) : 51.10 +- 21.95 (57.43, 25.86, 91.38, 57.60, 28.78, 67.43, 65.80, 54.56, 49.53, 12.62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APE with negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.131619691848755\n",
      "Epoch 2000: 1.9697465177159756e-05\n",
      "Epoch 3999: 1.5464081570826238e-06\n",
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert-jan/surfdrive/experiments/vit-position-info/toy-experiments/../analysis/learned_relative_position.py:170: RuntimeWarning: Mean of empty slice.\n",
      "  return sf.mean()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.138582706451416\n",
      "Epoch 2000: 4.7909285058267415e-05\n",
      "Epoch 3999: 3.915685738320462e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1145145893096924\n",
      "Epoch 2000: 2.8025293431710452e-05\n",
      "Epoch 3999: 2.897443209803896e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.0890612602233887\n",
      "Epoch 2000: 1.0324787581339478e-05\n",
      "Epoch 3999: 8.543328249288606e-07\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.168419599533081\n",
      "Epoch 2000: 9.13602070795605e-06\n",
      "Epoch 3999: 7.632702931914537e-07\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.3072509765625\n",
      "Epoch 2000: 2.534975646995008e-05\n",
      "Epoch 3999: 1.7185985825562966e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.3279130458831787\n",
      "Epoch 2000: 1.7819935237639584e-05\n",
      "Epoch 3999: 1.1457327673269901e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.104630947113037\n",
      "Epoch 2000: 3.1500516342930496e-05\n",
      "Epoch 3999: 2.4884907361411024e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1214351654052734\n",
      "Epoch 2000: 6.170295819174498e-05\n",
      "Epoch 3999: 4.798150257556699e-06\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.188685178756714\n",
      "Epoch 2000: 1.6439147657365538e-05\n",
      "Epoch 3999: 1.3195797237131046e-06\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 45.82 +- 17.15 (41.33, 47.81, 33.48, 62.65, 57.64, 12.57, 78.15, 32.24, 47.99, 44.32)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 40.29 +- 20.72 (46.22, 37.01, 40.47, 54.85, 31.96, 16.79, 78.80, 2.35, 33.16, 61.27)\n",
      "appearance (c2) : 34.51 +- 24.13 (9.83, 57.68, 32.53, 47.16, 2.88, 12.33, 72.43, 8.81, 64.83, 36.65)\n",
      "appearance (c3) : 42.25 +- 19.39 (43.27, 70.98, 52.28, 51.76, 41.11, 21.74, 69.33, 8.23, 42.29, 21.50)\n",
      "appearance (c4) : 72.12 +- 10.59 (85.04, 84.77, 76.11, 70.15, 52.82, 74.60, 76.32, 79.32, 67.37, 54.71)\n",
      "appearance (c5) : 72.42 +- 11.49 (88.60, 64.18, 48.16, 70.53, 65.58, 68.63, 75.44, 88.82, 79.84, 74.40)\n",
      "appearance (c6) : 72.61 +- 21.32 (87.64, 90.18, 66.80, 77.20, 81.23, 82.80, 86.27, 15.91, 82.33, 55.79)\n",
      "appearance (c7) : 81.62 +- 9.14 (87.42, 93.73, 80.62, 63.07, 76.16, 90.85, 82.25, 77.67, 91.60, 72.76)\n",
      "position (all): 54.18 +- 17.15 (58.67, 52.19, 66.52, 37.35, 42.36, 87.43, 21.85, 67.76, 52.01, 55.68)\n",
      "position (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "position (c1) : 59.71 +- 20.72 (53.78, 62.99, 59.53, 45.15, 68.04, 83.21, 21.20, 97.65, 66.84, 38.73)\n",
      "position (c2) : 65.49 +- 24.13 (90.17, 42.32, 67.47, 52.84, 97.12, 87.67, 27.57, 91.19, 35.17, 63.35)\n",
      "position (c3) : 57.75 +- 19.39 (56.73, 29.02, 47.72, 48.24, 58.89, 78.26, 30.67, 91.77, 57.71, 78.50)\n",
      "position (c4) : 27.88 +- 10.59 (14.96, 15.23, 23.89, 29.85, 47.18, 25.40, 23.68, 20.68, 32.63, 45.29)\n",
      "position (c5) : 27.58 +- 11.49 (11.40, 35.82, 51.84, 29.47, 34.42, 31.37, 24.56, 11.18, 20.16, 25.60)\n",
      "position (c6) : 27.39 +- 21.32 (12.36, 9.82, 33.20, 22.80, 18.77, 17.20, 13.73, 84.09, 17.67, 44.21)\n",
      "position (c7) : 18.38 +- 9.14 (12.58, 6.27, 19.38, 36.93, 23.84, 9.15, 17.75, 22.33, 8.40, 27.24)\n",
      "\n",
      "With bias:\n",
      "bias (all): 52.95 +- 17.80 (56.38, 53.02, 39.69, 65.45, 70.72, 6.86, 56.58, 70.05, 49.51, 61.25)\n",
      "bias (c0) : 85.02 +- 7.34 (73.78, 75.76, 94.15, 81.21, 90.42, 84.53, 80.45, 82.41, 90.69, 96.80)\n",
      "bias (c1) : 76.28 +- 12.55 (88.93, 81.39, 87.06, 61.28, 79.42, 77.31, 46.63, 88.77, 75.63, 76.35)\n",
      "bias (c2) : 74.26 +- 11.73 (85.01, 83.69, 72.00, 74.16, 80.81, 86.58, 46.13, 81.45, 64.98, 67.76)\n",
      "bias (c3) : 69.55 +- 10.80 (73.03, 58.33, 69.70, 52.40, 77.78, 72.68, 57.44, 87.30, 82.51, 64.30)\n",
      "bias (c4) : 53.67 +- 9.15 (52.36, 39.13, 39.61, 50.69, 55.17, 54.95, 59.71, 61.14, 71.56, 52.36)\n",
      "bias (c5) : 56.60 +- 9.63 (60.76, 67.44, 65.04, 54.09, 34.19, 57.38, 56.92, 65.71, 45.47, 58.96)\n",
      "bias (c6) : 54.98 +- 17.35 (41.64, 65.58, 57.83, 53.91, 38.29, 45.46, 25.56, 79.11, 57.60, 84.77)\n",
      "bias (c7) : 42.78 +- 15.23 (31.70, 23.88, 34.05, 68.57, 62.43, 20.79, 46.07, 54.66, 36.62, 48.99)\n",
      "appearance (all): 21.58 +- 7.48 (23.27, 26.19, 19.44, 24.11, 22.96, 10.15, 38.05, 11.65, 22.91, 17.05)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 11.39 +- 11.66 (5.12, 6.99, 5.24, 21.24, 6.58, 3.81, 42.06, 0.26, 8.08, 14.50)\n",
      "appearance (c2) : 10.96 +- 11.42 (1.47, 9.41, 9.11, 12.19, 0.55, 1.66, 39.01, 1.63, 22.70, 11.82)\n",
      "appearance (c3) : 14.24 +- 9.72 (11.67, 29.57, 15.84, 24.64, 9.13, 5.90, 29.51, 1.04, 7.39, 7.68)\n",
      "appearance (c4) : 33.67 +- 9.51 (40.51, 51.60, 45.96, 34.59, 23.68, 33.61, 30.75, 30.82, 19.16, 26.06)\n",
      "appearance (c5) : 31.43 +- 7.92 (34.77, 20.90, 16.83, 32.38, 43.16, 29.25, 32.50, 30.46, 43.53, 30.53)\n",
      "appearance (c6) : 35.22 +- 17.93 (51.15, 31.04, 28.17, 35.58, 50.12, 45.16, 64.23, 3.32, 34.90, 8.51)\n",
      "appearance (c7) : 47.94 +- 16.88 (59.71, 71.35, 53.16, 19.82, 28.61, 71.96, 44.36, 35.22, 58.06, 37.11)\n",
      "position (all): 25.47 +- 21.59 (20.34, 20.79, 40.87, 10.44, 6.32, 82.99, 5.37, 18.31, 27.58, 21.69)\n",
      "position (c0) : 14.98 +- 7.34 (26.22, 24.24, 5.85, 18.79, 9.58, 15.47, 19.55, 17.59, 9.31, 3.20)\n",
      "position (c1) : 12.34 +- 4.04 (5.96, 11.62, 7.71, 17.48, 14.00, 18.88, 11.31, 10.97, 16.29, 9.16)\n",
      "position (c2) : 14.79 +- 3.85 (13.52, 6.90, 18.89, 13.65, 18.64, 11.76, 14.85, 16.91, 12.32, 20.43)\n",
      "position (c3) : 16.21 +- 5.58 (15.30, 12.09, 14.46, 22.96, 13.08, 21.42, 13.05, 11.65, 10.09, 28.02)\n",
      "position (c4) : 12.66 +- 4.95 (7.13, 9.27, 14.42, 14.72, 21.15, 11.44, 9.54, 8.04, 9.28, 21.58)\n",
      "position (c5) : 11.97 +- 5.33 (4.47, 11.65, 18.12, 13.53, 22.65, 13.37, 10.58, 3.84, 10.99, 10.51)\n",
      "position (c6) : 9.81 +- 3.80 (7.21, 3.38, 14.00, 10.50, 11.58, 9.38, 10.22, 17.56, 7.49, 6.72)\n",
      "position (c7) : 9.29 +- 2.84 (8.59, 4.77, 12.78, 11.61, 8.96, 7.25, 9.57, 10.12, 5.32, 13.89)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TriViTal(\n",
       "  (patch_layer): Conv2d(3, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (block1): Block(\n",
       "    (attention_norm): LayerNorm((4,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_norm): LayerNorm((4,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn): Mlp(\n",
       "      (fc1): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (fc2): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (attn): Attention(\n",
       "      (query): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (key): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (value): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (out): Linear(in_features=4, out_features=4, bias=True)\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool1d(output_size=[1])\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=4, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'absolute'\n",
    "use_rel_pos = False\n",
    "\n",
    "# DEBUG\n",
    "d = 4\n",
    "n_heads = 2\n",
    "# seeds = range(2)\n",
    "# n_epochs = 1000\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n, attribution_method='input_gradient_withnegative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 1.9633574485778809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: 1.7992828361457214e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.9576891660690308\n",
      "Epoch 2000: 1.2932840945723001e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.968553900718689\n",
      "Epoch 2000: 6.610296259168535e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.030362606048584\n",
      "Epoch 2000: 2.0790374037460424e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.9532794952392578\n",
      "Epoch 2000: 1.0924077287199907e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.9364579916000366\n",
      "Epoch 2000: 0.00024555737036280334\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.0859546661376953\n",
      "Epoch 2000: 1.4142684449325316e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.011018753051758\n",
      "Epoch 2000: 1.2712179341178853e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.03263783454895\n",
      "Epoch 2000: 1.2904926734336186e-05\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.019577980041504\n",
      "Epoch 2000: 1.9887429516529664e-05\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 52.13 +- 13.69 (28.15, 69.43, 37.92, 64.76, 42.85, 48.50, 44.97, 54.85, 56.24, 73.63)\n",
      "appearance (c0) : 1.33 +- 4.00 (0.00, 0.00, 13.35, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 12.85 +- 26.19 (1.04, 1.03, 47.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 79.35)\n",
      "appearance (c2) : 41.82 +- 31.33 (48.65, 69.28, 52.77, 78.95, 0.00, 0.00, 0.00, 24.66, 62.93, 80.95)\n",
      "appearance (c3) : 65.23 +- 21.92 (55.82, 70.68, 62.61, 88.58, 9.75, 52.72, 78.44, 67.27, 77.18, 89.26)\n",
      "appearance (c4) : 82.15 +- 11.74 (56.15, 91.08, 72.87, 93.41, 82.88, 69.18, 91.58, 91.91, 82.82, 89.62)\n",
      "appearance (c5) : 88.67 +- 10.60 (60.61, 95.05, 81.28, 95.56, 96.57, 91.07, 93.29, 94.92, 83.53, 94.80)\n",
      "appearance (c6) : 92.34 +- 10.27 (63.24, 97.27, 86.63, 99.01, 97.02, 93.50, 97.59, 98.28, 94.47, 96.39)\n",
      "position (all): 47.87 +- 13.69 (71.85, 30.57, 62.08, 35.24, 57.15, 51.50, 55.03, 45.15, 43.76, 26.37)\n",
      "position (c0) : 98.67 +- 4.00 (100.00, 100.00, 86.65, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "position (c1) : 87.15 +- 26.19 (98.96, 98.97, 52.96, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 20.65)\n",
      "position (c2) : 58.18 +- 31.33 (51.35, 30.72, 47.23, 21.05, 100.00, 100.00, 100.00, 75.34, 37.07, 19.05)\n",
      "position (c3) : 34.77 +- 21.92 (44.18, 29.32, 37.39, 11.42, 90.25, 47.28, 21.56, 32.73, 22.82, 10.74)\n",
      "position (c4) : 17.85 +- 11.74 (43.85, 8.92, 27.13, 6.59, 17.12, 30.82, 8.42, 8.09, 17.18, 10.38)\n",
      "position (c5) : 11.33 +- 10.60 (39.39, 4.95, 18.72, 4.44, 3.43, 8.93, 6.71, 5.08, 16.47, 5.20)\n",
      "position (c6) : 7.66 +- 10.27 (36.76, 2.73, 13.37, 0.99, 2.98, 6.50, 2.41, 1.72, 5.53, 3.61)\n",
      "\n",
      "With bias:\n",
      "bias (all): 63.30 +- 13.88 (40.04, 52.02, 51.18, 69.74, 82.41, 84.16, 76.05, 65.12, 56.89, 55.37)\n",
      "bias (c0) : 85.43 +- 9.85 (87.97, 88.63, 67.97, 91.93, 68.59, 94.71, 96.95, 91.35, 88.52, 77.67)\n",
      "bias (c1) : 84.83 +- 11.12 (59.70, 91.69, 83.85, 92.46, 94.93, 79.58, 98.56, 93.00, 76.69, 77.88)\n",
      "bias (c2) : 73.97 +- 10.54 (77.01, 63.32, 78.30, 81.53, 75.11, 79.51, 90.38, 79.76, 53.75, 61.00)\n",
      "bias (c3) : 68.43 +- 16.80 (63.33, 38.27, 75.95, 73.86, 92.55, 81.70, 91.67, 58.15, 53.11, 55.74)\n",
      "bias (c4) : 60.37 +- 17.35 (43.50, 28.29, 52.49, 66.70, 71.42, 85.26, 86.34, 63.45, 58.87, 47.43)\n",
      "bias (c5) : 45.65 +- 13.94 (36.50, 16.80, 56.01, 54.91, 63.93, 57.91, 33.96, 57.26, 41.00, 38.18)\n",
      "bias (c6) : 30.94 +- 13.29 (32.54, 9.10, 32.06, 20.03, 52.01, 55.98, 31.78, 25.61, 25.30, 24.95)\n",
      "appearance (all): 25.41 +- 9.75 (17.79, 45.51, 19.69, 27.27, 15.12, 13.59, 20.92, 25.51, 30.54, 38.10)\n",
      "appearance (c0) : 0.43 +- 1.28 (0.00, 0.00, 4.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 3.02 +- 6.07 (0.42, 0.09, 11.77, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 17.91)\n",
      "appearance (c2) : 13.31 +- 11.50 (13.94, 25.93, 13.59, 14.58, 0.00, 0.00, 0.00, 4.99, 29.11, 30.95)\n",
      "appearance (c3) : 23.28 +- 13.52 (20.47, 42.76, 19.55, 23.15, 0.81, 13.21, 6.54, 28.15, 36.19, 41.96)\n",
      "appearance (c4) : 32.11 +- 16.12 (27.49, 69.75, 22.35, 31.82, 23.69, 13.78, 12.51, 33.76, 38.86, 47.11)\n",
      "appearance (c5) : 44.39 +- 16.63 (35.66, 79.08, 23.21, 42.12, 34.99, 22.19, 61.61, 40.57, 49.29, 55.18)\n",
      "appearance (c6) : 62.38 +- 14.44 (42.66, 82.79, 58.86, 79.17, 46.35, 40.10, 66.57, 73.11, 61.87, 72.34)\n",
      "position (all): 11.30 +- 12.94 (42.17, 2.47, 29.13, 2.98, 2.47, 2.24, 3.03, 9.37, 12.57, 6.53)\n",
      "position (c0) : 14.14 +- 9.16 (12.03, 11.37, 27.75, 8.07, 31.41, 5.29, 3.05, 8.65, 11.48, 22.33)\n",
      "position (c1) : 12.15 +- 11.46 (39.88, 8.22, 4.38, 7.54, 5.07, 20.42, 1.44, 7.00, 23.31, 4.21)\n",
      "position (c2) : 12.73 +- 6.18 (9.06, 10.75, 8.11, 3.89, 24.89, 20.49, 9.62, 15.26, 17.14, 8.05)\n",
      "position (c3) : 8.29 +- 5.87 (16.20, 18.96, 4.50, 2.99, 6.65, 5.08, 1.80, 13.70, 10.70, 2.30)\n",
      "position (c4) : 7.51 +- 9.93 (29.01, 1.96, 25.16, 1.48, 4.89, 0.96, 1.15, 2.80, 2.28, 5.46)\n",
      "position (c5) : 9.96 +- 8.94 (27.84, 4.12, 20.77, 2.97, 1.08, 19.90, 4.43, 2.17, 9.72, 6.64)\n",
      "position (c6) : 6.68 +- 7.15 (24.80, 8.10, 9.09, 0.79, 1.64, 3.92, 1.64, 1.28, 12.83, 2.71)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'absolute'\n",
    "use_rel_pos = False\n",
    "\n",
    "# DEBUG\n",
    "d = 4\n",
    "n_heads = 2\n",
    "# seeds = range(2)\n",
    "# n_epochs = 1000\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPE with negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.2573471069335938\n",
      "Epoch 999: 1.3175413608551025\n",
      "Accuracy: 0.5\n",
      "Epoch 0: 2.2481844425201416\n",
      "Epoch 999: 1.1509394645690918\n",
      "Accuracy: 0.5\n",
      "Epoch 0: 2.614542007446289\n",
      "Epoch 999: 1.2908046245574951\n",
      "Accuracy: 0.75\n",
      "Epoch 0: 2.159088373184204\n",
      "Epoch 999: 1.287803292274475\n",
      "Accuracy: 0.625\n",
      "Epoch 0: 2.497723340988159\n",
      "Epoch 999: 1.317810297012329\n",
      "Accuracy: 0.75\n",
      "Epoch 0: 2.1751880645751953\n",
      "Epoch 999: 1.2921916246414185\n",
      "Accuracy: 0.625\n",
      "Epoch 0: 2.1160402297973633\n",
      "Epoch 999: 1.2504820823669434\n",
      "Accuracy: 0.5\n",
      "Epoch 0: 2.2936601638793945\n",
      "Epoch 999: 1.184583067893982\n",
      "Accuracy: 0.875\n",
      "Epoch 0: 2.2572295665740967\n",
      "Epoch 999: 1.3673357963562012\n",
      "Accuracy: 0.75\n",
      "Epoch 0: 2.2111501693725586\n",
      "Epoch 999: 1.3387439250946045\n",
      "Accuracy: 0.5\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 87.50 +- 0.00 (87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50, 87.50)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c2) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c3) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c5) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c6) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c7) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "relative_position (all): 12.50 +- 0.00 (12.50, 12.50, 12.50, 12.50, 12.50, 12.50, 12.50, 12.50, 12.50, 12.50)\n",
      "relative_position (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "relative_position (c1) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c7) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 70.44 +- 4.74 (77.38, 70.30, 70.22, 78.46, 65.51, 73.29, 65.19, 72.53, 64.24, 67.32)\n",
      "bias (c0) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "bias (c1) : 75.93 +- 5.19 (83.22, 76.73, 75.39, 84.22, 69.51, 78.84, 68.92, 77.78, 69.30, 75.39)\n",
      "bias (c2) : 75.43 +- 5.24 (81.77, 76.25, 77.02, 83.59, 71.69, 79.75, 69.48, 77.27, 67.53, 69.91)\n",
      "bias (c3) : 75.75 +- 5.55 (84.16, 74.42, 76.35, 84.19, 72.21, 79.46, 69.49, 78.96, 68.44, 69.88)\n",
      "bias (c4) : 61.05 +- 5.14 (68.83, 61.64, 60.19, 70.46, 54.03, 63.06, 56.97, 62.38, 55.97, 56.96)\n",
      "bias (c5) : 62.31 +- 5.14 (70.27, 60.45, 58.64, 71.17, 58.20, 64.56, 56.11, 65.54, 56.51, 61.66)\n",
      "bias (c6) : 61.81 +- 5.04 (68.77, 62.18, 62.01, 70.67, 54.76, 64.34, 57.31, 63.46, 55.46, 59.09)\n",
      "bias (c7) : 51.28 +- 7.51 (62.03, 50.71, 52.15, 63.36, 43.71, 56.30, 43.25, 54.81, 40.76, 45.68)\n",
      "appearance (all): 29.56 +- 4.74 (22.62, 29.70, 29.78, 21.54, 34.49, 26.71, 34.81, 27.47, 35.76, 32.68)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 24.07 +- 5.19 (16.78, 23.27, 24.61, 15.78, 30.49, 21.16, 31.08, 22.22, 30.70, 24.61)\n",
      "appearance (c2) : 24.57 +- 5.24 (18.23, 23.75, 22.98, 16.41, 28.31, 20.25, 30.52, 22.73, 32.47, 30.09)\n",
      "appearance (c3) : 24.25 +- 5.55 (15.84, 25.58, 23.65, 15.81, 27.79, 20.54, 30.51, 21.04, 31.56, 30.12)\n",
      "appearance (c4) : 38.95 +- 5.14 (31.17, 38.36, 39.81, 29.54, 45.97, 36.94, 43.03, 37.61, 44.03, 43.04)\n",
      "appearance (c5) : 37.69 +- 5.14 (29.73, 39.55, 41.36, 28.83, 41.80, 35.44, 43.89, 34.46, 43.49, 38.34)\n",
      "appearance (c6) : 38.19 +- 5.04 (31.23, 37.82, 37.99, 29.32, 45.24, 35.66, 42.69, 36.54, 44.54, 40.91)\n",
      "appearance (c7) : 48.72 +- 7.51 (37.97, 49.29, 47.85, 36.64, 56.29, 43.70, 56.75, 45.19, 59.24, 54.32)\n",
      "relative_position (all): 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c7) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TriViTal(\n",
       "  (patch_layer): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (block1): Block(\n",
       "    (attention_norm): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_norm): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn): Mlp(\n",
       "      (fc1): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (fc2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (attn): Attention(\n",
       "      (query): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (key): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (value): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (out): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (rel_pos): RelPosEmb2D(\n",
       "        (emb_w): RelPosEmb1D()\n",
       "        (emb_h): RelPosEmb1D()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool1d(output_size=[1])\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=1, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = True\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n, attribution_method='input_gradient_withnegative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.0602939128875732\n",
      "Epoch 999: 1.2310948371887207\n",
      "Accuracy: 0.7142857313156128\n",
      "Epoch 0: 2.1744534969329834\n",
      "Epoch 999: 1.049857258796692\n",
      "Accuracy: 0.8571428656578064\n",
      "Epoch 0: 2.107271671295166\n",
      "Epoch 999: 1.2532310485839844\n",
      "Accuracy: 0.7142857313156128\n",
      "Epoch 0: 2.3134031295776367\n",
      "Epoch 999: 1.2634371519088745\n",
      "Accuracy: 0.5714285969734192\n",
      "Epoch 0: 2.1736080646514893\n",
      "Epoch 999: 1.0571417808532715\n",
      "Accuracy: 0.8571428656578064\n",
      "Epoch 0: 2.1035001277923584\n",
      "Epoch 999: 1.2126173973083496\n",
      "Accuracy: 0.5714285969734192\n",
      "Epoch 0: 2.023394823074341\n",
      "Epoch 999: 1.1563782691955566\n",
      "Accuracy: 0.7142857313156128\n",
      "Epoch 0: 1.9744210243225098\n",
      "Epoch 999: 1.1142261028289795\n",
      "Accuracy: 0.8571428656578064\n",
      "Epoch 0: 2.1626136302948\n",
      "Epoch 999: 1.2030061483383179\n",
      "Accuracy: 0.5714285969734192\n",
      "Epoch 0: 2.3069372177124023\n",
      "Epoch 999: 1.1143248081207275\n",
      "Accuracy: 0.8571428656578064\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 95.71 +- 12.86 (100.00, 100.00, 100.00, 57.14, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c0) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c1) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c2) : 90.00 +- 30.00 (100.00, 100.00, 100.00, 0.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c3) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c4) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c5) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "appearance (c6) : 100.00 +- 0.00 (100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "relative_position (all): 4.29 +- 12.86 (0.00, 0.00, 0.00, 42.86, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 10.00 +- 30.00 (0.00, 0.00, 0.00, 100.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 10.00 +- 30.00 (0.00, 0.00, 0.00, 100.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 10.00 +- 30.00 (0.00, 0.00, 0.00, 100.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 57.44 +- 6.25 (65.05, 56.31, 55.22, 70.84, 52.24, 62.45, 49.58, 54.81, 54.86, 53.08)\n",
      "bias (c0) : 71.48 +- 11.82 (74.42, 65.27, 65.90, 100.00, 64.27, 87.33, 63.54, 65.15, 63.98, 64.92)\n",
      "bias (c1) : 68.68 +- 11.09 (70.07, 63.83, 65.90, 100.00, 61.00, 73.71, 63.54, 63.61, 63.98, 61.18)\n",
      "bias (c2) : 66.56 +- 11.66 (70.07, 61.74, 65.90, 100.00, 60.27, 64.98, 57.10, 60.78, 63.98, 60.81)\n",
      "bias (c3) : 59.92 +- 3.59 (66.65, 60.85, 58.63, 54.87, 57.26, 64.98, 55.28, 60.41, 60.77, 59.51)\n",
      "bias (c4) : 54.63 +- 5.42 (65.95, 55.72, 55.49, 53.04, 48.42, 56.81, 46.35, 53.91, 60.19, 50.47)\n",
      "bias (c5) : 43.36 +- 6.58 (56.37, 46.05, 39.35, 51.04, 40.07, 44.67, 30.99, 42.19, 43.35, 39.53)\n",
      "bias (c6) : 37.46 +- 6.60 (51.85, 40.72, 35.33, 36.91, 34.42, 44.67, 30.25, 37.63, 27.72, 35.11)\n",
      "appearance (all): 42.56 +- 6.25 (34.95, 43.69, 44.78, 29.16, 47.76, 37.55, 50.42, 45.19, 45.14, 46.92)\n",
      "appearance (c0) : 28.52 +- 11.82 (25.58, 34.73, 34.09, 0.00, 35.73, 12.67, 36.46, 34.85, 36.02, 35.08)\n",
      "appearance (c1) : 31.32 +- 11.09 (29.93, 36.17, 34.10, 0.00, 39.00, 26.29, 36.46, 36.39, 36.02, 38.82)\n",
      "appearance (c2) : 33.44 +- 11.66 (29.93, 38.26, 34.10, 0.00, 39.73, 35.02, 42.90, 39.22, 36.02, 39.19)\n",
      "appearance (c3) : 40.08 +- 3.59 (33.35, 39.15, 41.37, 45.13, 42.74, 35.02, 44.72, 39.59, 39.23, 40.49)\n",
      "appearance (c4) : 45.37 +- 5.42 (34.05, 44.28, 44.51, 46.96, 51.58, 43.19, 53.65, 46.09, 39.81, 49.53)\n",
      "appearance (c5) : 56.64 +- 6.58 (43.63, 53.95, 60.65, 48.95, 59.93, 55.33, 69.01, 57.81, 56.65, 60.47)\n",
      "appearance (c6) : 62.54 +- 6.60 (48.15, 59.28, 64.67, 63.09, 65.58, 55.33, 69.75, 62.37, 72.28, 64.89)\n",
      "relative_position (all): 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%autoreload\n",
    "\n",
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'none'\n",
    "use_rel_pos = True\n",
    "\n",
    "# DEBUG\n",
    "n_epochs = 1000\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APE+RPE with negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.4138574600219727\n",
      "Epoch 2000: 0.8546656370162964\n",
      "Epoch 3999: 0.4172601103782654\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2340047359466553\n",
      "Epoch 2000: 0.9180114269256592\n",
      "Epoch 3999: 0.3845413625240326\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.333524465560913\n",
      "Epoch 2000: 0.9163439869880676\n",
      "Epoch 3999: 0.4042641222476959\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.2556543350219727\n",
      "Epoch 2000: 0.8919985294342041\n",
      "Epoch 3999: 0.5127609968185425\n",
      "Accuracy: 0.75\n",
      "Epoch 0: 2.606844425201416\n",
      "Epoch 2000: 0.8600109815597534\n",
      "Epoch 3999: 0.4508415460586548\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1695401668548584\n",
      "Epoch 2000: 0.8925485610961914\n",
      "Epoch 3999: 0.432542085647583\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.236496686935425\n",
      "Epoch 2000: 0.9062634706497192\n",
      "Epoch 3999: 0.44296571612358093\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.249708652496338\n",
      "Epoch 2000: 0.9172953367233276\n",
      "Epoch 3999: 0.40405145287513733\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1285531520843506\n",
      "Epoch 2000: 0.9613385796546936\n",
      "Epoch 3999: 0.3916335999965668\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1732969284057617\n",
      "Epoch 2000: 0.8599750995635986\n",
      "Epoch 3999: 0.4345833361148834\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 74.21 +- 6.90 (75.32, 77.15, 63.55, 65.08, 81.66, 85.35, 67.85, 78.29, 77.96, 69.90)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 81.34 +- 7.70 (74.24, 87.00, 84.40, 67.12, 77.23, 92.08, 72.96, 81.84, 88.67, 87.88)\n",
      "appearance (c2) : 86.40 +- 4.65 (82.79, 93.43, 87.27, 79.03, 81.08, 87.44, 85.13, 88.32, 94.29, 85.22)\n",
      "appearance (c3) : 83.77 +- 8.24 (85.83, 91.80, 76.87, 79.67, 67.19, 93.65, 81.84, 90.60, 92.85, 77.43)\n",
      "appearance (c4) : 91.55 +- 3.34 (88.50, 95.44, 92.26, 85.32, 88.47, 94.90, 89.39, 92.35, 96.05, 92.87)\n",
      "appearance (c5) : 90.89 +- 3.90 (89.94, 94.71, 89.78, 85.63, 84.47, 96.35, 87.81, 93.40, 95.41, 91.44)\n",
      "appearance (c6) : 92.18 +- 3.29 (91.57, 96.21, 91.09, 88.49, 86.36, 95.60, 91.10, 94.51, 96.72, 90.19)\n",
      "appearance (c7) : 94.28 +- 2.36 (93.22, 96.98, 94.07, 90.68, 90.68, 97.09, 92.82, 95.60, 97.39, 94.27)\n",
      "position (all): 25.76 +- 6.86 (24.68, 22.85, 36.20, 34.92, 18.33, 14.65, 32.15, 21.71, 22.04, 30.10)\n",
      "position (c0) : 99.98 +- 0.05 (100.00, 100.00, 99.83, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00, 100.00)\n",
      "position (c1) : 18.59 +- 7.73 (25.76, 13.00, 14.95, 32.88, 22.77, 7.92, 27.04, 18.16, 11.33, 12.12)\n",
      "position (c2) : 13.56 +- 4.66 (17.21, 6.57, 12.31, 20.97, 18.92, 12.56, 14.87, 11.68, 5.71, 14.78)\n",
      "position (c3) : 16.19 +- 8.21 (14.17, 8.20, 22.73, 20.33, 32.81, 6.35, 18.16, 9.40, 7.15, 22.57)\n",
      "position (c4) : 8.42 +- 3.35 (11.50, 4.56, 7.49, 14.68, 11.53, 5.10, 10.61, 7.65, 3.95, 7.13)\n",
      "position (c5) : 9.08 +- 3.89 (10.06, 5.29, 9.98, 14.37, 15.53, 3.65, 12.19, 6.60, 4.59, 8.56)\n",
      "position (c6) : 7.80 +- 3.28 (8.43, 3.79, 8.70, 11.51, 13.64, 4.40, 8.90, 5.49, 3.28, 9.81)\n",
      "position (c7) : 5.71 +- 2.36 (6.78, 3.02, 5.85, 9.32, 9.32, 2.91, 7.18, 4.40, 2.61, 5.73)\n",
      "relative_position (all): 0.03 +- 0.07 (0.00, 0.00, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.02 +- 0.05 (0.00, 0.00, 0.17, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.06 +- 0.19 (0.00, 0.00, 0.65, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.04 +- 0.12 (0.00, 0.00, 0.41, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.04 +- 0.12 (0.00, 0.00, 0.40, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.02 +- 0.07 (0.00, 0.00, 0.25, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.02 +- 0.07 (0.00, 0.00, 0.23, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.02 +- 0.06 (0.00, 0.00, 0.20, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c7) : 0.01 +- 0.02 (0.00, 0.00, 0.08, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 52.81 +- 5.47 (57.10, 59.08, 53.57, 58.94, 53.30, 51.55, 41.53, 46.18, 57.24, 49.59)\n",
      "bias (c0) : 89.09 +- 6.10 (89.27, 94.40, 91.05, 87.82, 89.33, 97.11, 74.11, 86.60, 95.03, 86.22)\n",
      "bias (c1) : 62.73 +- 12.01 (77.02, 72.83, 58.87, 73.40, 48.78, 47.55, 54.61, 75.04, 72.72, 46.48)\n",
      "bias (c2) : 55.66 +- 8.87 (60.21, 56.14, 62.61, 64.20, 49.80, 63.08, 35.11, 48.12, 53.34, 64.03)\n",
      "bias (c3) : 59.21 +- 10.64 (51.15, 73.12, 65.57, 63.49, 60.38, 51.10, 60.23, 33.98, 67.87, 65.18)\n",
      "bias (c4) : 44.59 +- 9.30 (45.18, 57.12, 40.02, 55.75, 35.52, 43.68, 45.33, 29.28, 57.75, 36.21)\n",
      "bias (c5) : 46.16 +- 9.28 (47.70, 50.58, 61.83, 55.22, 45.26, 33.71, 32.40, 34.96, 47.60, 52.36)\n",
      "bias (c6) : 41.62 +- 6.23 (51.71, 44.47, 45.02, 49.76, 35.82, 35.08, 30.61, 40.40, 42.33, 41.00)\n",
      "bias (c7) : 32.90 +- 7.46 (35.82, 34.19, 39.03, 40.82, 41.27, 38.09, 18.86, 23.00, 32.29, 25.59)\n",
      "appearance (all): 39.45 +- 5.49 (36.72, 36.09, 33.55, 29.81, 42.34, 46.56, 43.74, 48.16, 37.95, 39.53)\n",
      "appearance (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "appearance (c1) : 30.59 +- 11.07 (17.06, 23.64, 34.72, 17.85, 39.55, 48.30, 33.12, 20.43, 24.19, 47.04)\n",
      "appearance (c2) : 38.35 +- 8.03 (32.94, 40.98, 32.63, 28.29, 40.70, 32.28, 55.24, 45.82, 44.00, 30.65)\n",
      "appearance (c3) : 34.37 +- 10.76 (41.93, 24.68, 26.46, 29.09, 26.62, 45.80, 32.55, 59.81, 29.83, 26.96)\n",
      "appearance (c4) : 50.70 +- 8.54 (48.51, 40.92, 55.37, 37.75, 57.05, 53.45, 48.87, 65.30, 40.59, 59.24)\n",
      "appearance (c5) : 49.02 +- 9.19 (47.04, 46.80, 34.27, 38.35, 46.24, 63.87, 59.36, 60.75, 50.00, 43.57)\n",
      "appearance (c6) : 53.82 +- 6.03 (44.22, 53.43, 50.08, 44.46, 55.42, 62.06, 63.21, 56.32, 55.77, 53.21)\n",
      "appearance (c7) : 63.30 +- 7.46 (59.83, 63.83, 57.35, 53.67, 53.25, 60.10, 75.32, 73.61, 65.94, 70.14)\n",
      "position (all): 7.74 +- 4.07 (6.18, 4.83, 12.77, 11.25, 4.36, 1.88, 14.73, 5.66, 4.81, 10.88)\n",
      "position (c0) : 10.90 +- 6.10 (10.73, 5.60, 8.93, 12.18, 10.67, 2.89, 25.89, 13.40, 4.97, 13.78)\n",
      "position (c1) : 6.65 +- 3.08 (5.92, 3.53, 6.15, 8.74, 11.66, 4.15, 12.27, 4.53, 3.09, 6.49)\n",
      "position (c2) : 5.97 +- 2.32 (6.85, 2.88, 4.60, 7.51, 9.50, 4.64, 9.65, 6.06, 2.66, 5.32)\n",
      "position (c3) : 6.41 +- 3.08 (6.92, 2.20, 7.83, 7.42, 13.00, 3.11, 7.22, 6.21, 2.30, 7.86)\n",
      "position (c4) : 4.70 +- 1.87 (6.30, 1.96, 4.47, 6.50, 7.43, 2.87, 5.80, 5.41, 1.67, 4.55)\n",
      "position (c5) : 4.81 +- 2.15 (5.26, 2.62, 3.81, 6.44, 8.50, 2.42, 8.24, 4.29, 2.40, 4.08)\n",
      "position (c6) : 4.55 +- 2.03 (4.07, 2.10, 4.79, 5.78, 8.75, 2.86, 6.18, 3.27, 1.89, 5.79)\n",
      "position (c7) : 3.79 +- 1.48 (4.35, 1.99, 3.57, 5.52, 5.48, 1.80, 5.82, 3.39, 1.77, 4.26)\n",
      "relative_position (all): 0.01 +- 0.03 (0.00, 0.00, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.02, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.03 +- 0.08 (0.00, 0.00, 0.27, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.02 +- 0.05 (0.00, 0.00, 0.16, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.01 +- 0.04 (0.00, 0.00, 0.14, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.01 +- 0.04 (0.00, 0.00, 0.15, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.01 +- 0.03 (0.00, 0.00, 0.09, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.01 +- 0.03 (0.00, 0.00, 0.11, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c7) : 0.01 +- 0.01 (0.00, 0.00, 0.05, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TriViTal(\n",
       "  (patch_layer): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (block1): Block(\n",
       "    (attention_norm): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn_norm): LayerNorm((1,), eps=1e-06, elementwise_affine=True)\n",
       "    (ffn): Mlp(\n",
       "      (fc1): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (fc2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (attn): Attention(\n",
       "      (query): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (key): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (value): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (out): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (proj_dropout): Dropout(p=0.0, inplace=False)\n",
       "      (softmax): Softmax(dim=-1)\n",
       "      (rel_pos): RelPosEmb2D(\n",
       "        (emb_w): RelPosEmb1D()\n",
       "        (emb_h): RelPosEmb1D()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pool): AdaptiveAvgPool1d(output_size=[1])\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc): Linear(in_features=1, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'absolute'\n",
    "use_rel_pos = True\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n, attribution_method='input_gradient_withnegative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APE+RPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 2.39790940284729\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2000: 0.7713456749916077\n",
      "Epoch 3999: 0.27304694056510925\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.952418565750122\n",
      "Epoch 2000: 0.80018550157547\n",
      "Epoch 3999: 0.31930211186408997\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.3168041706085205\n",
      "Epoch 2000: 0.6673423051834106\n",
      "Epoch 3999: 0.30945900082588196\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1713151931762695\n",
      "Epoch 2000: 0.8037557601928711\n",
      "Epoch 3999: 0.3111928701400757\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.684062957763672\n",
      "Epoch 2000: 0.7276705503463745\n",
      "Epoch 3999: 0.31806719303131104\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.1811769008636475\n",
      "Epoch 2000: 0.717259407043457\n",
      "Epoch 3999: 0.24621827900409698\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 1.997013807296753\n",
      "Epoch 2000: 0.7446480393409729\n",
      "Epoch 3999: 0.2645026743412018\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.0889101028442383\n",
      "Epoch 2000: 0.8055406212806702\n",
      "Epoch 3999: 0.28729942440986633\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.323491334915161\n",
      "Epoch 2000: 0.6426935195922852\n",
      "Epoch 3999: 0.26373720169067383\n",
      "Accuracy: 1.0\n",
      "Epoch 0: 2.0047848224639893\n",
      "Epoch 2000: 0.6567929983139038\n",
      "Epoch 3999: 0.33350488543510437\n",
      "Accuracy: 1.0\n",
      "\n",
      "Without bias:\n",
      "appearance (all): 93.79 +- 6.36 (99.93, 98.00, 99.49, 85.71, 95.47, 89.37, 99.90, 84.73, 100.00, 85.34)\n",
      "appearance (c0) : 59.43 +- 39.35 (82.68, 87.54, 94.04, 0.00, 72.64, 92.89, 85.39, 0.00, 79.13, 0.00)\n",
      "appearance (c1) : 87.54 +- 6.36 (82.68, 92.79, 96.64, 78.22, 82.79, 92.89, 85.39, 94.49, 79.13, 90.42)\n",
      "appearance (c2) : 88.37 +- 6.43 (82.68, 95.22, 97.04, 78.22, 88.19, 92.89, 85.39, 94.49, 79.13, 90.42)\n",
      "appearance (c3) : 94.01 +- 6.48 (99.94, 95.22, 97.04, 78.22, 88.19, 96.64, 99.94, 94.49, 100.00, 90.42)\n",
      "appearance (c4) : 98.97 +- 1.10 (99.97, 97.76, 97.04, 100.00, 99.88, 98.25, 99.97, 97.79, 100.00, 99.08)\n",
      "appearance (c5) : 99.14 +- 1.01 (99.98, 97.76, 97.04, 100.00, 99.88, 98.83, 99.98, 98.52, 100.00, 99.39)\n",
      "appearance (c6) : 99.22 +- 0.87 (99.98, 97.76, 97.80, 100.00, 99.88, 98.83, 99.98, 98.52, 100.00, 99.39)\n",
      "position (all): 4.78 +- 5.98 (0.07, 2.00, 0.51, 0.00, 4.53, 10.63, 0.10, 15.27, 0.00, 14.66)\n",
      "position (c0) : 40.57 +- 39.35 (17.32, 12.46, 5.96, 100.00, 27.36, 7.11, 14.61, 100.00, 20.87, 100.00)\n",
      "position (c1) : 12.46 +- 6.36 (17.32, 7.21, 3.36, 21.78, 17.21, 7.11, 14.61, 5.51, 20.87, 9.58)\n",
      "position (c2) : 11.63 +- 6.43 (17.32, 4.78, 2.96, 21.78, 11.81, 7.11, 14.61, 5.51, 20.87, 9.58)\n",
      "position (c3) : 5.99 +- 6.48 (0.06, 4.78, 2.96, 21.78, 11.81, 3.36, 0.06, 5.51, 0.00, 9.58)\n",
      "position (c4) : 1.03 +- 1.10 (0.03, 2.24, 2.96, 0.00, 0.12, 1.75, 0.03, 2.21, 0.00, 0.92)\n",
      "position (c5) : 0.86 +- 1.01 (0.02, 2.24, 2.96, 0.00, 0.12, 1.17, 0.02, 1.48, 0.00, 0.61)\n",
      "position (c6) : 0.78 +- 0.87 (0.02, 2.24, 2.20, 0.00, 0.12, 1.17, 0.02, 1.48, 0.00, 0.61)\n",
      "relative_position (all): 1.43 +- 4.29 (0.00, 0.00, 0.00, 14.29, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n",
      "With bias:\n",
      "bias (all): 41.83 +- 6.79 (47.13, 46.21, 41.39, 51.38, 42.78, 40.69, 29.74, 32.38, 49.64, 37.00)\n",
      "bias (c0) : 87.74 +- 12.49 (87.36, 65.19, 93.78, 95.16, 63.01, 98.26, 88.32, 99.35, 89.25, 97.75)\n",
      "bias (c1) : 52.68 +- 6.17 (53.95, 65.84, 59.42, 52.76, 51.36, 52.82, 43.31, 46.90, 53.71, 46.69)\n",
      "bias (c2) : 41.86 +- 9.38 (43.00, 55.69, 44.24, 44.21, 56.95, 40.92, 25.64, 31.40, 43.27, 33.29)\n",
      "bias (c3) : 36.95 +- 7.26 (43.00, 39.27, 24.79, 44.21, 44.14, 40.92, 25.64, 30.91, 43.27, 33.29)\n",
      "bias (c4) : 29.53 +- 9.37 (33.48, 39.27, 24.72, 36.96, 44.14, 25.19, 15.15, 16.96, 36.54, 22.92)\n",
      "bias (c5) : 20.21 +- 12.93 (31.63, 30.59, 22.62, 36.07, 18.52, 14.96, 0.03, 0.83, 36.51, 10.33)\n",
      "bias (c6) : 14.85 +- 9.77 (23.42, 22.64, 16.04, 27.19, 13.50, 10.46, 0.02, 0.55, 27.58, 7.10)\n",
      "appearance (all): 57.59 +- 6.67 (52.86, 52.96, 58.54, 48.62, 55.13, 58.26, 70.23, 66.35, 50.36, 62.54)\n",
      "appearance (c0) : 10.59 +- 11.50 (12.63, 30.48, 6.04, 0.00, 32.62, 1.68, 11.67, 0.00, 10.75, 0.00)\n",
      "appearance (c1) : 40.83 +- 5.96 (38.07, 32.52, 38.16, 36.95, 35.34, 43.82, 48.41, 50.18, 36.63, 48.20)\n",
      "appearance (c2) : 51.96 +- 8.24 (47.13, 43.32, 54.11, 43.64, 43.00, 54.88, 63.50, 64.82, 44.89, 60.32)\n",
      "appearance (c3) : 56.78 +- 9.01 (47.13, 59.37, 72.98, 43.64, 55.79, 54.88, 63.50, 65.28, 44.89, 60.32)\n",
      "appearance (c4) : 69.84 +- 9.20 (66.51, 59.37, 73.05, 63.04, 55.79, 73.94, 84.83, 81.82, 63.46, 76.61)\n",
      "appearance (c5) : 77.17 +- 13.38 (68.35, 64.41, 74.78, 63.93, 67.46, 83.55, 99.94, 96.98, 63.49, 88.85)\n",
      "appearance (c6) : 83.26 +- 10.09 (76.56, 73.66, 82.12, 72.81, 76.29, 88.49, 99.96, 97.98, 72.42, 92.33)\n",
      "position (all): 0.58 +- 0.68 (0.02, 0.83, 0.07, 0.00, 2.09, 1.05, 0.02, 1.27, 0.00, 0.46)\n",
      "position (c0) : 1.67 +- 1.97 (0.01, 4.34, 0.18, 4.84, 4.37, 0.06, 0.01, 0.65, 0.00, 2.25)\n",
      "position (c1) : 6.50 +- 3.75 (7.98, 1.63, 2.42, 10.29, 13.31, 3.35, 8.28, 2.93, 9.66, 5.11)\n",
      "position (c2) : 6.18 +- 4.45 (9.87, 0.99, 1.65, 12.15, 0.05, 4.20, 10.86, 3.78, 11.84, 6.39)\n",
      "position (c3) : 6.28 +- 4.35 (9.87, 1.36, 2.23, 12.15, 0.07, 4.20, 10.86, 3.81, 11.84, 6.39)\n",
      "position (c4) : 0.63 +- 0.73 (0.01, 1.36, 2.23, 0.00, 0.07, 0.87, 0.02, 1.23, 0.00, 0.47)\n",
      "position (c5) : 2.62 +- 4.09 (0.02, 5.00, 2.60, 0.00, 14.02, 1.49, 0.03, 2.20, 0.00, 0.82)\n",
      "position (c6) : 1.89 +- 2.99 (0.02, 3.70, 1.85, 0.00, 10.22, 1.04, 0.02, 1.47, 0.00, 0.57)\n",
      "relative_position (all): 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c0) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c1) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c2) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c3) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c4) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c5) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "relative_position (c6) : 0.00 +- 0.00 (0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train / hybrid\n",
    "seeds = range(10)\n",
    "n_epochs = 4000\n",
    "report_every_n = 2000\n",
    "lr = 4e-3\n",
    "d = 1\n",
    "n_heads = 1\n",
    "n_classes = len(COLORS)\n",
    "pos_emb = 'absolute'\n",
    "use_rel_pos = True\n",
    "\n",
    "run_appearance(seeds, n_epochs, lr, d, n_heads, n_classes, pos_emb, use_rel_pos, train_images, train_labels, test_images, test_labels, analysis_images, analysis_labels, report_every_n=report_every_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('vit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a95f2993495c6408642ff11835c089ad7be69b3fd867463ffeddef912e9cf373"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
